{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSi1BC1tigaPeggwaPAVMA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/Data_Analysis_Machine_Learning/blob/master/3.%20Risk%20Management%20and%20Financial%20Analysis/Real%20Estate%20Utah/GridSearchCV_Hyperparameter_Codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, we can expand the parameter grids for each regression model to include more parameters. This will help in fine-tuning the models further. Here's an updated version of the previous examples with additional parameters:\n",
        "\n",
        "### 1. Ridge Regression\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Define the model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_ridge = {\n",
        "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
        "    'fit_intercept': [True, False],\n",
        "    'normalize': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_ridge = GridSearchCV(estimator=ridge, param_grid=param_grid_ridge, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search_ridge.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best parameters for Ridge Regression: \", grid_search_ridge.best_params_)\n",
        "print(\"Best cross-validation score for Ridge Regression: \", grid_search_ridge.best_score_)\n",
        "```\n",
        "\n",
        "### 2. Lasso Regression\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Define the model\n",
        "lasso = Lasso()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_lasso = {\n",
        "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
        "    'selection': ['cyclic', 'random'],\n",
        "    'fit_intercept': [True, False],\n",
        "    'normalize': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_lasso = GridSearchCV(estimator=lasso, param_grid=param_grid_lasso, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search_lasso.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best parameters for Lasso Regression: \", grid_search_lasso.best_params_)\n",
        "print(\"Best cross-validation score for Lasso Regression: \", grid_search_lasso.best_score_)\n",
        "```\n",
        "\n",
        "### 3. Elastic Net Regression\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Define the model\n",
        "elastic_net = ElasticNet()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_en = {\n",
        "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
        "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "    'fit_intercept': [True, False],\n",
        "    'normalize': [True, False],\n",
        "    'selection': ['cyclic', 'random']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_en = GridSearchCV(estimator=elastic_net, param_grid=param_grid_en, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search_en.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best parameters for Elastic Net Regression: \", grid_search_en.best_params_)\n",
        "print(\"Best cross-validation score for Elastic Net Regression: \", grid_search_en.best_score_)\n",
        "```\n",
        "\n",
        "### 4. Decision Tree Regression\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Define the model\n",
        "decision_tree = DecisionTreeRegressor()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_dt = {\n",
        "    'criterion': ['mse', 'friedman_mse', 'mae'],\n",
        "    'splitter': ['best', 'random'],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': [None, 'auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_dt = GridSearchCV(estimator=decision_tree, param_grid=param_grid_dt, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best parameters for Decision Tree Regression: \", grid_search_dt.best_params_)\n",
        "print(\"Best cross-validation score for Decision Tree Regression: \", grid_search_dt.best_score_)\n",
        "```\n",
        "\n",
        "### 5. Random Forest Regression\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Define the model\n",
        "random_forest = RandomForestRegressor()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_rf = GridSearchCV(estimator=random_forest, param_grid=param_grid_rf, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best parameters for Random Forest Regression: \", grid_search_rf.best_params_)\n",
        "print(\"Best cross-validation score for Random Forest Regression: \", grid_search_rf.best_score_)\n",
        "```\n",
        "\n",
        "### 6. Gradient Boosting Regression\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Define the model\n",
        "gbr = GradientBoostingRegressor()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_gbr = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'max_features': [None, 'auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_gbr = GridSearchCV(estimator=gbr, param_grid=param_grid_gbr, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search_gbr.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best parameters for Gradient Boosting Regression: \", grid_search_gbr.best_params_)\n",
        "print(\"Best cross-validation score for Gradient Boosting Regression: \", grid_search_gbr.best_score_)\n",
        "```\n",
        "\n",
        "### 7. Support Vector Regression (SVR)\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Define the model\n",
        "svr = SVR()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_svr = {\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'C': [0.1, 1, 10, 100, 1000],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'degree': [2, 3, 4, 5],  # Relevant only for polynomial kernel\n",
        "    'epsilon': [0.1, 0.2, 0.5, 0.3, 1]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_svr = GridSearchCV(estimator=svr, param_grid=param_grid_svr, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search_svr.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best parameters for SVR: \", grid_search_svr.best_params_)\n",
        "print(\"Best cross-validation score for SVR: \", grid_search_svr.best_score_)\n",
        "```\n",
        "\n",
        "### Summary\n",
        "\n",
        "This expanded version includes more hyperparameters for each regression model, providing a more comprehensive search space for GridSearchCV to explore. This approach increases the chances of finding the optimal combination of hyperparameters for each model, potentially improving their performance on the dataset.\n",
        "\n",
        "When running these code snippets, make sure to replace `X_train` and `y_train` with your actual training data. Additionally, the search space defined in the parameter grids can be adjusted based on specific needs and computational resources."
      ],
      "metadata": {
        "id": "ecusuoZbeA9v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEVL07nKd6aP"
      },
      "outputs": [],
      "source": []
    }
  ]
}