# -*- coding: utf-8 -*-
"""diamonds-eda-price-prediction_V6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12x-RLy1T7PJO4PCaXbxp7P0ZVBu1ITGL

<h1 align = "center" > Diamonds Dataset Analysis And Price Predictions </h1>

<div style="text-align: center;">
    <a href="https://ibb.co/Wzz1L38"><img src="https://i.ibb.co/YddGxWn/pink-diamonds-australia-supercontinent.webp" alt="pink-diamonds-australia-supercontinent" border="0"></a>
</div>

<a id=0></a>
[Dataset Link](https://www.kaggle.com/datasets/sahilnbajaj/diamonds-sale-data/data)
# Project Content
1. [Introduction](#1)
    * 1.1 [Examining the Project Topic](#2)
    * 1.2 [Recognizing Variables In Dataset](#3)
        * 1.2.1 [GIA Color Scale](#4)
        * 1.2.2 [Clarity Grades of Diamonds](#5)
        * 1.2.3 [Cut's Grade of Diamonds](#6)
2. [First Organization](#7)
    * 2.1 [Required Python Libraries](#8)
    * 2.2 [Loading The Dataset](#9)
    * 2.3 [Initial Analysis on The Dataset](#10)
        * 2.3.1 [Analysis Output](#11)
    * 2.4 [Examining Missing Values](#12)
    * 2.5 [Examining Unique Values](#13)
        * 2.5.1 [Analysis Output](#14)
    * 2.6 [Seperating Numeric and Categorical Features](#15)
3. [Statistical Analysis Of Features](#16)
    * 3.1 [Numeric Features Analysis](#17)
    * 3.2 [Categorical Features Analysis](#18)
        * 3.2.1 [Analysis Output](#19)
4. [Examining Missing Values in Depth](#20)
    * 4.1 [Dimensions (x, y, z) Evaluation](#21)
    * 4.2 [Remove Anomalies](#22)
5. [Bi-Variate Analysis(EDA)](#23)
    * 5.1 [Bi-Variate Analysis Of Numeric Features](#24)
        * 5.1.1 [Analysis Output](#25)
    * 5.2 [Bi-Variate Analysis Of Categorical Features ](#26)
        * 5.2.1 [Bi-Variate Using Barplot](#27)
        * 5.2.2 [Bivariate Using Pointplot](#29)
            * 5.2.2.1 [Analysis Output](#30)
    * 5.3 [Correlation Heatmap](#31)
        * 5.3.1 [LabelEncoding for Categorical Features](#32)
            * 5.3.1.1 [Correlation Heatmap With Feature's LabelEncoding](#33)
        * 5.3.2 [One-Hot Encoding for Categorical Features](#34)
            * 5.3.2.1 [Correlation Heatmap With Feature's One-Hot Encoded](#35)
        * 5.3.3 [TargetEncoding For Categorical Features](#36)
            * 5.3.3.1 [Correlation Heatmap With Feature's Target Encoded](#37)
    * 5.4 [Categorical Feature's Encoding on Original Dataframe](#38)
6. [Preparing DataFrame for Modelling](#39)
    * 6.1 [Dropping Features With Low Correlation](#40)
    * 6.2 [Outlier Detection](#41)
        * 6.2.1 [Analysis Output](#44)
        * 6.2.2 [Handling Outliers](#42)
            * 6.2.2.1 [Transforming price & carat](#45)
            * 6.2.2.2 [Removing Other Outliers](#46)
    * 6.3 [Data Scaling](#43)
        * 6.3.1 [Standardization](#47)
7. [Machine Learning Modelling](#48)
    * 7.1 [Splitting Data into Training & Test Sets](#49)
    * 7.2 [Modelling](#50)
        * 7.2.1 [Linear Regression](#51)
            * 7.2.1.1 [Linear Regression Learning Curve](#52)
            * 7.2.1.2 [Linear Regression Model Performance](#53)
            * 7.2.1.3 [Analysis Output](#54)
        * 7.2.2 [Polynomial Regression](#55)
            * 7.2.2.1 [Polynomial Regression Learning Curve](#56)
            * 7.2.2.2 [Polynomial Regression Model Performance](#57)
            * 7.2.2.3 [Analysis Output](#58)
        * 7.2.3 [Ridge Regression](#59)
            * 7.2.3.1 [Ridge Regression Learning Curve](#60)
            * 7.2.3.2 [Ridge Regression Model Performance](#61)
            * 7.2.3.3 [Analysis Output](#62)
        * 7.2.4 [ElasticNet Regression](#63)
            * 7.2.4.1 [Hyperparameter Optimization](#64)
            * 7.2.4.2 [ElasticNet Model With Tuned Parameter](#65)
            * 7.2.4.3 [ElasticNet Learning Curve](#66)
            * 7.2.4.4 [ElasticNet Model Performance](#67)
            * 7.2.4.5 [Analysis Output](#68)

## 1. Introduction <a id=1></a>

### 1.1 Examining the Project Topic <a id=2></a>

#### The price of diamonds is determined based on what criteria?

The price of diamonds is determined based on several key criteria that collectively influence their value. These criteria include:

1. **Carat Weight**: The weight of the diamond, measured in carats, is one of the most significant factors influencing its price. Generally, as the carat weight increases, so does the price, assuming other factors are equal.

2. **Cut Quality**: The cut of a diamond refers to its proportions, symmetry, and polish, which directly impact its brilliance, fire, and overall visual appeal. Diamonds with excellent cut grades typically command higher prices due to their superior sparkle and beauty.

3. **Color Grade**: The presence or absence of color in a diamond affects its price. Colorless diamonds (graded D to F) are rarer and thus more valuable than diamonds with visible color (graded G to Z). Fancy colored diamonds, which exhibit colors other than yellow or brown, are also valued based on the intensity and rarity of their color.

4. **Clarity Grade**: Clarity refers to the presence of internal flaws (inclusions) and external blemishes (blemishes) in a diamond. Diamonds with higher clarity grades, indicating fewer and less visible imperfections, tend to be more valuable because of their rarity and perceived purity.

5. **Shape**: Different diamond shapes, such as round, princess, emerald, or pear, can impact the price based on consumer preferences and market demand. Round diamonds, for example, are typically more expensive due to their popularity and the amount of rough diamond lost during the cutting process to achieve the round shape.

6. **Certification**: Diamonds that have been certified by reputable gemological laboratories, such as the Gemological Institute of America (GIA) or the American Gem Society (AGS), may command higher prices due to the assurance of their quality and characteristics provided by the certification.

7. **Market Trends and Demand**: Like any commodity, the price of diamonds can also be influenced by market trends, consumer demand, and economic factors. Fluctuations in supply and demand, changes in consumer preferences, and global economic conditions can all impact diamond prices.

### 1.2 Recognizing Variables In Dataset <a id=3></a>

#### Diamond Specification:

1. **Carat**: Carat is the unit of measurement for the weight of a diamond. One carat is equivalent to 200 milligrams.

2. **Cut**: The cut of a diamond refers to its proportions, symmetry, and polish. **It's a critical factor in determining a diamond's brilliance and fire**.

3. **Color**: Diamond color refers to the presence or absence of color in a diamond. The Gemological Institute of America (GIA) grades diamond color on a scale from D (colorless) to Z (light yellow or brown).

4. **Clarity**: Clarity measures the presence of internal flaws (inclusions) and external blemishes (blemishes) in a diamond. The GIA grades clarity on a scale from Flawless (no inclusions or blemishes visible under 10x magnification) to Included (inclusions and/or blemishes visible to the naked eye).

5. **Depth**: Depth is the height of a diamond from the culet to the table. It's expressed as a percentage of the diamond's overall diameter.

6. **Table**: The table is the large, flat facet on the top of a diamond. Table percentage refers to the width of the table facet as a percentage of the diamond's overall diameter.

7. **Price**: Price refers to the cost of the diamond, which is influenced by various factors including carat weight, cut, color, and clarity.

8. **x, y, z**: These dimensions represent the length, width, and depth of the diamond, respectively. They are typically measured in millimeters.

#### 1.2.1 GIA Color Scales <a id=4></a>

1. **D - Colorless**: Diamonds in the D category are completely colorless, with no traces of yellow or brown. They are extremely rare and highly valued for their purity.

2. **E - Colorless**: E-grade diamonds are also considered colorless, with very minute traces of color that are difficult to detect even to the trained eye. They are also highly valued for their purity and brilliance.

3. **F - Colorless**: F-grade diamonds are nearly colorless, with extremely faint traces of color that are usually only detectable under controlled lighting conditions. They are still considered very high quality and appear colorless to the naked eye.

4. **G - Near Colorless**: G-grade diamonds have slight traces of color, typically a subtle yellowish or brownish hue. However, this coloration is usually not noticeable to the untrained eye, especially when the diamond is set in jewelry.

5. **H - Near Colorless**: H-grade diamonds have slightly more visible color than G-grade diamonds, but the color is still relatively faint. They offer a good balance between quality and value, as they tend to be less expensive than higher grade diamonds.

6. **I - Near Colorless**: I-grade diamonds have noticeable traces of color, usually a faint yellowish or brownish tint. While the color may be detectable when viewed closely, I-grade diamonds still offer good value for those seeking a larger diamond on a budget.

7. **J - Near Colorless to Faint Yellow**: J-grade diamonds have more noticeable color than I-grade diamonds, with a faint yellowish or brownish tint. The color may be more apparent, especially in larger diamonds, but J-grade diamonds can still be an attractive option for those prioritizing size and budget.

#### 1.2.2 Clarity Grades of Diamonds <a id=5></a>

1. **IF (Internally Flawless)**: This is the highest clarity grade. Diamonds in this category have no internal flaws (inclusions) visible under 10x magnification but may have minor surface blemishes.

2. **VVS1 (Very, Very Slightly Included 1)**: Diamonds in this grade have very, very small inclusions that are extremely difficult to detect even under 10x magnification. These diamonds are considered of very high clarity.

3. **VVS2 (Very, Very Slightly Included 2)**: Similar to VVS1, but with slightly more visible inclusions, though still very difficult to detect without magnification.

4. **VS1 (Very Slightly Included 1)**: VS1 diamonds have small inclusions that are visible under 10x magnification but are typically not visible to the naked eye. They are considered to have very good clarity.

5. **VS2 (Very Slightly Included 2)**: Similar to VS1, but with slightly more visible inclusions, though still generally not visible to the naked eye without magnification.

6. **SI1 (Slightly Included 1)**: SI1 diamonds have noticeable inclusions under 10x magnification, but these inclusions are typically not visible to the naked eye. They are considered to have good clarity.

7. **SI2 (Slightly Included 2)**: Similar to SI1, but with more noticeable inclusions that may be visible to the naked eye, especially in larger or higher clarity diamonds.

8. **I1 (Included 1)**: I1 diamonds have inclusions that are easily visible under 10x magnification and may be visible to the naked eye. They are considered to have noticeable clarity characteristics and are generally less valuable.

[More Information](https://shorturl.at/pBDEW)

#### 1.2.3 Cut's Grade of Diamonds <a id=6></a>

1. **Ideal**: Diamonds with an "Ideal" cut grade are crafted to precise proportions that maximize their brilliance and sparkle. They exhibit exceptional fire and scintillation, with perfect symmetry and polish. Ideal cut diamonds are highly sought after for their unparalleled beauty and are typically more expensive due to the precision required in their cutting.

2. **Premium**: Diamonds with a "Premium" cut grade are also well-crafted with good proportions and symmetry, resulting in excellent brilliance and sparkle. While not quite as precise as Ideal cut diamonds, Premium cut diamonds still offer exceptional beauty and are a popular choice for those seeking a balance between quality and value.

3. **Very Good**: Diamonds with a "Very Good" cut grade have slightly less precise proportions and symmetry compared to Ideal and Premium cut diamonds, but they still exhibit impressive brilliance and sparkle. They offer excellent value for those looking for high-quality diamonds at a more affordable price point.

4. **Good**: Diamonds with a "Good" cut grade have acceptable proportions and symmetry, but they may not maximize the stone's brilliance to the same extent as higher cut grades. While Good cut diamonds may still exhibit good sparkle, they are generally considered a lower-tier option compared to higher cut grades.

5. **Fair**: Diamonds with a "Fair" cut grade have proportions and symmetry that are less precise, resulting in diminished brilliance and sparkle. These diamonds may exhibit noticeable dullness or lackluster appearance compared to diamonds with higher cut grades. Fair cut diamonds are typically the least expensive option and are often chosen for budget-conscious buyers.

## 2. First Organization <a id=7></a>
[Go to Project Content](#0)

### 2.1 Basic Python Libraries <a id=8></a>
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""### 2.2 Loading The Dataset <a id=9></a>"""

df = pd.read_csv("/kaggle/input/diamonds-sale-data/diamonds.csv")

"""### 2.3 Initial Analysis on The Dataset <a id=10></a>"""

df.head()

df.info()

"""#### 2.3.1 Analysis Output <a id=11></a>

1. The datasets contains of 53940 rows and 10 columns.
2. The type of dataset's features are :  
    a. `carat` , `depth`, `table`, `x`, `y`, `z` are in float.  
    b. `price` is integer.  
    c. `cut`, `color` and `calrity` are objects.  
3. According to basic analysis there are no missing values in this dataset.

### 2.4 Examining Missing Values
"""

pd.DataFrame(df.isnull().sum(), columns=['Missing Values'])

import missingno


missingno.bar(df, color = "b", figsize=(10, 5), fontsize=12)
plt.show()

"""### 2.5 Examining Unique Values <a id=13><a/>"""

pd.DataFrame(df.nunique(), columns=["Number Of Unique Values"])

"""#### 2.5.1 Analysis Output <a id=14></a>

According to analsysis :
* *Categorical Features* : `cut`, 'color', `clarity` are categorical features
* *Numeric Features* : `carat`, `depth`, `table`, `price`, `x`, `y` and `z` are numeric features

### 2.6 Seperating Numeric and Categorical Features <a id=15></a>
[Go to Project Content](#0)
"""

df.head()

numeric_features = ["carat", "depth", "table", "price", "x", "y", "z"]
categorial_features = ['cut', 'color', 'clarity']

"""## 3. Statistical Analysis Of Features <a id=16></a>
[Go to Project Content](#0)
"""

df[numeric_features].describe().T

"""### 3.1 Features Analysis (Uni-Variate Analysis) <a id=17></a>"""

# carat analysis
sns.histplot(data=df['carat'], color='darkblue', bins='auto', kde=True)
plt.title("carat distribution", fontsize=10)

"""***Analysis of `carat` feature***:  
* The minimum value of carat is 0.2 and maximum is 5.01
* The mean of carat is 0.7979
* According to histplot it's seems that `carat` is right skewed.
"""

# depth
sns.histplot(data=df['depth'], color='darkblue', bins='auto', kde=True)
plt.title("depth distribution", fontsize=10)

"""***Analysis of `depth` feature***:
* The mean of `depth` is 61.74.
* The minimum is 43 and maxomum is 79.
* According to histplot it is clear that `depth` has been normally distributed.
"""

# table
sns.histplot(data=df['table'], color='darkblue', bins=20, kde=True)
plt.title("table distribution", fontsize=10)

"""***Analysis of `table` feature***:
* The average of `table ` is 57.45.
* minimum is 43 and maximum is 95.
* According to summary statistics table we can say that this feature has right skewness. It is shonw that 75 quantile is equal to 59 while maximum number os 95, it means that 75% of rows are less than Quantile 3.
"""

# price
sns.histplot(data=df['price'], color='darkblue', bins='auto', kde=True)
plt.title("price distribution", fontsize=10)

"""***Analysis of `price` feature***:
* The minimum amout is 326 and maximum is 18823.
* The average price is 3932
* According to statistical table and hisplot is is clear that this feature is right skewed.
"""

# x
sns.histplot(data=df['x'], color='darkblue', bins='auto', kde=True)
plt.title("x distribution", fontsize=10)

"""***Analysis of `x` feature***:
* The minimum amount of x is 0.0 and maximum is 10.74.
* The average of x is 5.73.
* According to histplot and since the mean is slightly greater than the median (Quantile 50%), the distribution of the x feature is slightly right-skewed.
"""

# y
sns.histplot(data=df['y'], color='darkblue', bins='auto', kde=True)
plt.title("y distribution", fontsize=10)

"""***Analysis of y feature***:
* The average is 5.73.
* Minumum amount os 0 and maximum is 58.90
* According to histplot and summary statistics table , mean and median (Q2) are almost equal, we can assume that `y` feature is normally distributed.
"""

# z
sns.histplot(data=df['z'], color='darkblue', bins='auto', kde=True)
plt.title("z distribution", fontsize=10)

"""***Analysis of z feature***:
* The minimum number is 0 and maximum is 31.80.
* Mean of z is 3.53
* As per hisplot and statistics table it is clear that this feature is normally distributed since median (Q2) and mean of this feature are almost equal.

### 3.2 Categorical Features Analysis <a id=18></a>
[Go to Project Content](#0)
"""

# Seleting categorical features
categorical = ['cut', 'color', 'clarity']
print(categorical)

# Plotting pie charts for each categorical column
for column in categorical:
    counts = df[column].value_counts()
    sns.color_palette("bright")
    plt.figure(figsize=(10, 6))
    plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140)
    plt.title(f'Pie chart for {column}', fontdict={"color" : "darkblue", "weight" : "bold", "size" : 15})
    plt.axis('equal')
    plt.legend()
    plt.show()

"""#### 3.2.1 Analysis Output <a id=19></a>
[Go to Project Content](#0)

***Cut Feature***:
1. About *40%* of diamonds has ideal cut and about *25%* has premium cut.
2. So more about **65%** of diamonds are good in terms of *Cut*.

***Color Feature***:
1. This features shows the color of each diamonds.
2. The color grade of D, E, F are colorless and they are highly valued.
3. About *48%* of diamonds are colorless with color grade of D, E, F.
4. Grades G, H, I and J have a little color, respectively, from G, which has a little color, to J, which has a more significant amount of color.
5. That does not mean that J Grade diamonds are cheaper, they can still be an attractive option for those prioritizing size and budget.

***Clarity Feature***:
1. Most common clarity grade in this dataset is *SI1* with about 24% of total diamonds.
2. *SI1* are considered to have good clarity.
3. Best clarity grade is **IF** which there are about 3% in this dataset. (They are rare type of diamonds)
4. **VVS1**, **VVS2** have small inclusion which are difficult to detect without magnification. They are considered to have very good clarity.
5. 16% of diamonds in this dataset are with **VVS1** & **VVS2** clarity grades.
6. **VS1**, **VS2** have small inclusion which are not visible to naked eye.
7. About 37% of diamonds in this dataset are with clarity grade of **VS1** & **VS2**.
8. **SI1** and **SI2** have about 41% of diamods in this dataset.
9. **L1** clarity grade have more visible inclusions. They are less than 2% in this dataset.
10. **L1** considered to be less valubale.

## 4. Examining Missing Values in Depth <a id=20></a>
[Go to Project Content](#0)

### 4.1 Dimensions (x, y, z) Evaluation <a id=21></a>
"""

df[(df['x'] == 0) | (df['y'] == 0 | (df['z'] == 0))]

"""**As per Describe() method** :
* Min value of x, y, z are 0.
* According above table , there 7 rows which has dimensions of 0 for all dimensions.
* It's unusual for diamonds to have dimensions of 0 for all three dimensions (x, y, z) while still being valuable. This discrepancy likely indicates an error or missing data in the dataset.
* These rows could represent outliers or anomalies in the dataset

### 4.2 Remove Anomalies <a id=22></a>
"""

df = df[~(df['x'] == 0) | (df['y'] == 0 | (df['z'] == 0))]
df.info()

# Resetting the index of df
df.reset_index(drop=True, inplace=True)

"""## 5. Bi-Variate Analysis (EDA)<a id=23></a>
[Go to Project Content](#0)
"""

# Seperating Numeric and Categoreical Variables
numeric_features = ["price", "depth", "table", "carat", 'x', 'y', 'z']
categorical_features = ["clarity", "cut", "color"]

"""### 5.1 Bi-Variate Analysis Of Numeric Features <a id=24></a>"""

def scatter(col):
    plt.figure(figsize=(8, 6))
    sns.set_style("darkgrid")
    sns.regplot(data=df, x=col, y='price', color='blue', line_kws={"color": 'black'})
    plt.title(f"{col} Vs Price", fontdict={"color" : "darkred", "weight" : "bold", "size" : 15})
    plt.xlabel(col, fontdict={"color": "darkblue", "weight": "bold", "size": 10})
    plt.ylabel('Price', fontdict={"color": "darkblue", "weight": "bold", "size": 10})
    plt.show()

scatter('carat')

scatter('depth')

scatter('table')

scatter('x')

scatter('y')

scatter('z')

# check numeric corellation
df[numeric_features].corr()

"""#### 5.1.1 Analysis Output <a id=25></a>

1. **carat**:
   - There is a ***strong positive*** correlation (0.92) between carat weight and price (target), indicating that as the carat weight of a diamond increases, its price tends to increase significantly.
   - Carat weight also has ***strong positive*** correlations with the dimensions x, y, and z, suggesting that larger diamonds tend to have greater carat weights.
   - The correlation between carat weight and table width is moderate (0.18), indicating a slight positive relationship between these two variables.

2. **depth**:
   - There is a weak negative correlation (-0.01) between depth and price, suggesting that there is little to no relationship between the depth of a diamond and its price.
   - Depth has a moderate negative correlation (-0.30) with table width, indicating that as the depth of a diamond increases, its table width tends to decrease.
   - Depth also has weak negative correlations with the dimensions x, y, and z, indicating that there is little relationship between depth and the physical dimensions of the diamond.

3. **table**:
   - There is a weak positive correlation (0.13) between table width and price, suggesting a slight positive relationship between these two variables.
   - Table width has a moderate positive correlation (0.20) with carat weight, indicating that larger diamonds tend to have wider table widths.
   - Table width also has weak positive correlations with the dimensions x, y, and z, suggesting a slight positive relationship between table width and the physical dimensions of the diamond.

4. **x, y, z**:
   - All three dimensions (x, y, and z) have strong positive correlations with carat weight (0.97 for x, 0.95 for y, and 0.95 for z), indicating that larger diamonds have greater dimensions in all three axes.
   - There are moderate to strong positive correlations between the dimensions (x, y, and z) and price, suggesting that larger diamonds tend to have higher prices.
   - The dimensions x, y, and z also have weak to moderate positive correlations with table width, indicating a slight positive relationship between these variables.

### 5.2 Bi-Variate Analysis Of Categorical Features <a id=26></a>

#### 5.2.1 Bi-Variate Using Barplot <a id=27></a>
"""

def bivariate_barplot(col):
    """This method would compare price by each categorical feature"""
    # Bar Plots with Aggregated Metrics
    mean = df.groupby(col)['price'].mean()
    plt.figure(figsize=(10, 6))
    sns.barplot(x=mean.index, y=mean.values, palette='rainbow')
    plt.title(f'Mean Price by {col}', fontdict={"color" : "darkred", "weight" : "bold", "size" : 15})
    plt.xlabel(col, fontdict={"color" : "darkblue", "weight" : "bold", "size" : 10})
    plt.ylabel('Mean Price', fontdict={"color" : "darkblue", "weight" : "bold", "size" : 10})
    plt.show()

# mean price by cut
bivariate_barplot('cut')

# mean price by clarity
bivariate_barplot('clarity')

# mean price by color
bivariate_barplot('color')

"""#### 5.2.2 Bivariate Using Pointplot <a id=29></a>"""

def pointplot(col):
    plt.figure(figsize=(10, 6))
    sns.pointplot(data=df, x=col, y='price', color='purple', ci=None)
    plt.title(f'Point plot of {col} vs Price', fontdict={"color" : "darkred", "weight" : "bold", "size" : 15})
    plt.xlabel(col, fontdict={"color" : "darkblue", "weight" : "bold", "size" : 10})
    plt.ylabel('Price', fontdict={"color" : "darkblue", "weight" : "bold", "size" : 10})
    plt.legend()
    plt.show()

# cut vs price
pointplot('cut')

# clarity vs price
pointplot('clarity')

# clarity vs price
pointplot('color')

"""#### 5.2.2.1 Analysis Output <a id=30></a>

**Average Price By Cut**:
* As per barplot of cut's grade its seems that most valued grade are **Premium** and **Fair**.
* It is somehow weired that average price of **Fair** cut is higher than **Very Good**.

**Average Price By Clarity**:
* Highest average price by clarity is for **SI2**.
* Unlike our imiganation , **IF** clarity grade does not have highest mean average while it has highest clarity grade and less inclusions.
* **VS1** and **VS2** grades are about 4000.
* Less average price by clarity grade is for **VVS1**

**Average Price By Color**:
*  **D** color grade which considered as colorless diamonds has average price of about 3100.
* We can understand that color grades of **D**, **E**, **F** and **G** are less valued compare to other categories.
* **J** color grade is near to colorless but they have more noticeable color prone to yellow or brown.
* Most valued color grade is **J**

### 5.3 Correlation Heatmap <a id=31></a>
[Go to Project Content](#0)

#### 5.3.1 LabelEncoding for Categorical Features <a id=32></a>
"""

from sklearn.preprocessing import LabelEncoder

# Creating an object of LabelEncoder Class
le = LabelEncoder()

# Creating new datafram from categorical feature for proceeding with labelencoding them.
df_categorical = df[categorial_features]

# encoding
for col in df_categorical.columns:
    df_categorical[col] = le.fit_transform(df_categorical[col])

# creating new dataframe
df_new = pd.concat([df[numeric_features], df_categorical], axis=1)
df_new.head()

"""#### 5.3.1.1 Correlation Heatmap With Feature's LabelEncoded <a id=33></a>"""

plt.figure(figsize=(10, 6))
sns.heatmap(df_new.corr(numeric_only=True), annot=True)

"""#### 5.3.2 One-Hot Encoding for Categorical Features <a id=34></a>"""

df_encoded = pd.get_dummies(df, columns=['cut', 'color', 'clarity'], drop_first=True, dtype=int)
df_encoded.head(2)

"""#### 5.3.2.1 Correlation Heatmap With Feature's One-Hot Encoded <a id=35></a>"""

plt.figure(figsize=(20, 10))
sns.heatmap(df_encoded.corr().T, annot=True, fmt="2.3f")

"""#### 5.3.3 TargetEncoding For Categorical Features <a id=36></a>"""

df_new_2 = df.copy()
df_new_2.head(1)

from category_encoders import TargetEncoder
encoder = TargetEncoder()

df_new_2['cut'] = encoder.fit_transform(df_new_2['cut'], df_new_2['price'])
df_new_2['clarity'] = encoder.fit_transform(df_new_2['clarity'], df_new_2['price'])
df_new_2['color'] = encoder.fit_transform(df_new_2['color'], df_new_2['price'])

"""#### 5.3.3.1 Correlation Heatmap With Feature's Target Encoded <a id=37></a>"""

plt.figure(figsize=(10, 6))
sns.heatmap(df_new_2.corr(numeric_only=True), annot=True)

"""**Strong positive correlations between price and carat, dimensions (x, y, z), indicating that larger diamonds tend to have higher prices.
Weak positive correlations between price and cut quality, color, and clarity, suggesting that higher cut quality, better color grades, and higher clarity grades may be associated with slightly higher prices.**

### 5.4 Categorical Feature's Encoding on Original Dataframe <a id=38></a>
"""

target_encoder = TargetEncoder()

for col in categorial_features:
    df[col] = target_encoder.fit_transform(df[col], df['price'])

# check the DataFrame
df.head(2)

"""## 6. Preparing DataFrame for Modelling <a id=39></a>
[Go to Project Content](#0)

### 6.1 Dropping Features With Low Correlation <a id=40></a>
"""

# Choosing appropriate features for modeling

df_model = df[['carat', 'color', 'clarity', 'price', 'x', 'y', 'z']].copy()
df_model.head(1) # checking the new DataFrame

"""### 6.2 Outlier Detection <a id=41></a>"""

fig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8)) = plt.subplots(2, 4, figsize=(20, 12))

ax1.boxplot(df_model["carat"], showmeans=True)
ax1.set_title("carat")

ax2.boxplot(df_model["color"], showmeans=True)
ax2.set_title("color")

ax3.boxplot(df_model["clarity"], showmeans=True)
ax3.set_title("clarity")

ax4.boxplot(df_model["price"], showmeans=True)
ax4.set_title("price")

ax5.boxplot(df_model["x"], showmeans=True)
ax5.set_title("x")

ax6.boxplot(df_model["y"], showmeans=True)
ax6.set_title("y")

ax7.boxplot(df_model["z"], showmeans=True)
ax7.set_title("z")

# # Hide the extra subplot
ax8.axis('off')

plt.tight_layout()
plt.show()

"""#### 6.2.1 Analysis Output <a id=44></a>
* As per above boxplot we can easily see that there are lots of outliers specially in `carat` and `price` features.
* Other features such as `x`, `y`, `z` and`clarity` have less outliers.
* `color` features does not have any outlier as per boxplot.
* Since 'price' and 'carat' have many outliers, it's essential to handle them carefully to avoid distorting the model's performance.
* the best approach to handle outliers would be a combination of techniques based on the feature characteristics and the impact of outliers on the target variable (price)

#### 6.2.2 Handling Outliers <a id=42></a>

#### 6.2.2.1 Transforming `price` & `carat` <a id=45></a>
**Applying a logarithmic transformation can help reduce the impact of outliers in `price` & `carat` features**
"""

df_model['price'] = np.log1p(df_model['price'])
df_model['carat'] = np.log1p(df_model['carat'])

df_model.head(2)

# Let's check outliers by using box plot again
plt.boxplot(data=df_model, x='price')

# Boxplot for carat
plt.boxplot(data=df_model, x='carat')

"""#### 6.2.2.2 Removing Other Outliers <a id=46></a>"""

def remove_outliers(df_model, column_name):
    """Remove outliers based on IQR"""
    Q1 = np.quantile(df_model[column_name], 0.25)
    Q3 = np.quantile(df_model[column_name], 0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_model = df_model[(df_model[column_name] >= lower_bound) & (df_model[column_name] <= upper_bound)]
    return df_model

for col in df_model.columns:
    df_model = remove_outliers(df_model, col)

# Reseting Index
df_model.reset_index(inplace=True, drop=True)
df_model.info()

"""### 6.3 Data Scaling <a id=43></a>
[Go to Project Content](#0)

#### 6.3.1 Standardization <a id=47></a>
"""

from sklearn.preprocessing import StandardScaler

# Standardization of DataFrame
standard_scaler = StandardScaler()
for col in df_model.columns:
    df_model[col] = standard_scaler.fit_transform(df_model[[col]])

# check dataframe
df_model.head(2)

"""## 7. Machine Learning Modelling <a id=48></a>
[Go to Project Content](#0)

### 7.1 Splitting Data into Training & Test Sets <a id=49></a>
"""

from sklearn.model_selection import train_test_split

# Seleting Features
X = df_model.drop(['price'], axis=1)

# Selecting Traget
y = df_model['price']

# Split data to train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check train and test data
print("X_train: ", X_train.shape)
print("X_test: ", X_test.shape)
print("y_train: ", y_train.shape)
print("y_test: ", y_test.shape)

"""### 7.2 Modelling <a id=50></a>"""

# Importing accuracy methods
from sklearn.metrics import (
    r2_score,
    accuracy_score,
    mean_squared_error
        )
# learning curve method
from sklearn.model_selection import learning_curve

# Model performance
from yellowbrick.regressor import PredictionError
from yellowbrick.regressor import ResidualsPlot

"""#### 7.2.1 Linear Regression <a id=51></a>"""

from sklearn.linear_model import LinearRegression

# Linear Regression Instance
lr_model = LinearRegression()

# Training
lr_model.fit(X_train, y_train)

# Prediction
y_pred = lr_model.predict(X_test)

# Accuracy
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

# Show results
print(f"Linear Regression R2: {r2}")
print(f"Linear Regression MSE: {mse}")

"""#### 7.2.1.1 Linear Regression Learning Curve <a id=52></a>"""

def plot_learning_curve(model, X, y):
    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5, scoring='neg_mean_squared_error')
    train_scores_mean = -np.mean(train_scores, axis=1)
    test_scores_mean = -np.mean(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training MSE")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation MSE")
    plt.xlabel("Training examples")
    plt.ylabel("MSE")
    plt.title("Learning Curves")
    plt.legend(loc="best")
    plt.show()

# Learning Curve for Linear Regression Model
plot_learning_curve(lr_model, X, y)

"""#### 7.2.1.2 Linear Regression Model Performance <a id=53></a>"""

def evaluate_regression_model(model, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):
    """
    Evaluate a regression model using the PredictionError visualizer from Yellowbrick.

    Args:
        model (object): A trained regression model.
        X_train (pandas.DataFrame or numpy.ndarray): Training data features.
        y_train (pandas.Series or numpy.ndarray): Training data target.
        X_test (pandas.DataFrame or numpy.ndarray): Testing data features.
        y_test (pandas.Series or numpy.ndarray): Testing data target.

    Returns:
        None
    """
    # Create the PredictionError visualizer
    visualizer = PredictionError(model)

    # Fit the visualizer on the training data
    visualizer.fit(X_train, y_train)

    # Score the visualizer on the testing data
    visualizer.score(X_test, y_test)

    # Display the visualizer
    visualizer.show()

evaluate_regression_model(lr_model)

def visualize_residuals(model,X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):
    """
    Visualize the residuals of a regression model using the ResidualsPlot from Yellowbrick.

    Args:
        model (object): A trained regression model.
        X_train (pandas.DataFrame or numpy.ndarray): Training data features.
        y_train (pandas.Series or numpy.ndarray): Training data target.
        X_test (pandas.DataFrame or numpy.ndarray): Testing data features.
        y_test (pandas.Series or numpy.ndarray): Testing data target.

    Returns:
        None
    """
    # Create the ResidualsPlot visualizer
    visualizer = ResidualsPlot(model)

    # Fit the visualizer on the training data
    visualizer.fit(X_train, y_train)

    # Score the visualizer on the testing data
    visualizer.score(X_test, y_test)

visualize_residuals(lr_model)

y_train_pred = lr_model.predict(X_train)
y_test_pred = lr_model.predict(X_test)

# Calculate test and train MSE
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)


# Show Results
print("Train MSE:", train_mse)
print("Test MSE:", test_mse)
print("Train R-squared:", train_r2)
print("Test R-squared:", test_r2)

"""#### 7.2.1.3 Analysis Output <a id=54></a>

1. **Train MSE (Mean Squared Error)** is 0.0314, which is relatively low, indicating a good fit on the training data.
2. **Test MSE** is 0.0340, which is only slightly higher than the Train MSE. A large difference between Train and Test MSE would indicate overfitting, but in this case, the difference is small.
3. **Train R-squared** is 0.9686, which is very close to 1, suggesting that the model explains most of the variance in the training data.
4. **Test R-squared** is 0.9659, which is also very high and close to the Train R-squared value. A significant drop in R-squared from Train to Test would indicate overfitting, but here the drop is minimal.

#### 7.2.2 Polynomial Regression <a id=55></a>
"""

from sklearn.preprocessing import PolynomialFeatures

# Creating instance of linear regression and polynomial
lr_pf_model = LinearRegression()
pf = PolynomialFeatures(degree=3)


# Training and test
X_train_pf = pf.fit_transform(X_train)
X_test_pf = pf.fit_transform(X_test)

# Training the Model
lr_pf_model.fit(X_train_pf, y_train)

# Prediction
y_pred = lr_pf_model.predict(X_test_pf)

# Accuracy
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
score = lr_pf_model.score(X_test_pf, y_test)

# Show results
print(f"Polynomial Regression R2: {r2}")
print(f"Polynomial Regression MSE: {mse}")
print(f"Polynomial Regression Score: {score}")

"""#### 7.2.2.1 Polynomial Regression Learning Curve <a id=56></a>"""

plot_learning_curve(lr_pf_model, X, y)

"""#### 7.2.2.2 Polynomial Regression Model Performance <a id=57></a>"""

evaluate_regression_model(lr_pf_model, X_train=X_train_pf, X_test=X_test_pf, y_train=y_train, y_test=y_test)

visualize_residuals(lr_pf_model, X_train=X_train_pf, X_test=X_test_pf, y_train=y_train, y_test=y_test)

y_train_pred = lr_pf_model.predict(X_train_pf)
y_test_pred = lr_pf_model.predict(X_test_pf)

# Calculate test and train MSE
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)


# Show Results
print("Train MSE:", train_mse)
print("Test MSE:", test_mse)
print("Train R-squared:", train_r2)
print("Test R-squared:", test_r2)

"""#### 7.2.2.3 Analysis Output <a id=58></a>

1. Train MSE (Mean Squared Error) is 0.0111, which is relatively low, indicating a good fit on the training data.
2. Test MSE is 0.0125, which is only slightly higher than the Train MSE. A large difference between Train and Test MSE would indicate overfitting, but in this case, the difference is small.
3. Train R-squared is 0.9889, which is very close to 1, suggesting that the model explains most of the variance in the training data.
4. Test R-squared is 0.9875, which is also very high and close to the Train R-squared value. A significant drop in R-squared from Train to Test would indicate overfitting, but here the drop is minimal.

#### 7.2.3 Ridge Regression <a id=50></a>
"""

from sklearn.linear_model import Ridge

# Creating Ridge Instance
ridge_model = Ridge()

# Training the Model
ridge_model.fit(X_train, y_train)

# Prediction
y_pred = ridge_model.predict(X_test)

# Accuracy Scores
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)


# Show results
print(f"Ridge Regression R2: {r2}")
print(f"Ridge Regression MSE: {mse}")

"""#### 7.2.3.1 Ridge Regression Learning Curve <a id=60></a>"""

plot_learning_curve(ridge_model, X, y)

"""#### 7.2.3.2 Ridge Regression Model Performance <a id=61></a>"""

evaluate_regression_model(ridge_model)

visualize_residuals(ridge_model)

y_train_pred = ridge_model.predict(X_train)
y_test_pred = ridge_model.predict(X_test)

# محاسبه خطای آموزش و آزمون
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

# نمایش نتایج
print("Train MSE:", train_mse)
print("Test MSE:", test_mse)
print("Train R-squared:", train_r2)
print("Test R-squared:", test_r2)

"""#### 7.2.3.3 Analysis Output <a id=62></a>

1. Train MSE (Mean Squared Error) is 0.0315, which is relatively low, indicating a good fit on the training data.
2. Test MSE is 0.0344, which is only slightly higher than the Train MSE. A large difference between Train and Test MSE would indicate overfitting, but in this case, the difference is small.
3. Train R-squared is 0.9685, which is very close to 1, suggesting that the model explains most of the variance in the training data.
4. Test R-squared is 0.9658, which is also very high and close to the Train R-squared value. A significant drop in R-squared from Train to Test would indicate overfitting, but here the drop is minimal.

#### 7.2.4 ElasticNet Regression <a id=63></a>
[Go to Project Content](#0)
"""

from sklearn.linear_model import ElasticNet

# Creating ElasticNet Instance
elastic_net = ElasticNet()

# Traning The Model
elastic_net.fit(X_train, y_train)

# Prediction
y_pred = elastic_net.predict(X_test)

# Accuracy Scores
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)


# Show results
print(f"Elastic Regression R2: {r2}")
print(f"Elastic Regression MSE: {mse}")

"""#### 7.2.4.1 Hyperparameter Optimization <a id=64></a>
*Hyperparameter Optimization Using RandomSearch*
"""

# import
from sklearn.model_selection import RandomizedSearchCV

# Creating parameteres dictionary
params = {
            'alpha': [1e-4,1e-3,1e-2,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],   # Learning Rate
            'l1_ratio': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],                     # Whether use Ridge Or Lasso
            'fit_intercept': [True, False],                                    # Whether to fit an intercept
            'precompute': [True, False],                                       # Whether to precompute Gram matrix
            'max_iter': [1000, 2000, 3000],                                    # Maximum number of iterations
            'selection': ['cyclic', 'random']                                 # Method for subproblem optimization
            }

# Perform random search using RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=elastic_net, param_distributions=params, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
random_search.fit(X_train, y_train)

# Show hyperparameter results
print(f"Best Parameters: {random_search.best_params_}")
print(f"Best R2 Score: {random_search.best_score_}")

"""#### 7.2.4.2 ElasticNet Model With Tuned Parameter <a id=65></a>"""

# Creating ElasticNet Instance
elastic_net_tuned = ElasticNet(selection='cyclic', max_iter=2000, l1_ratio=0.4, fit_intercept=False, alpha=0.001, precompute=False)

# Traning The Model
elastic_net_tuned.fit(X_train, y_train)

# Prediction
y_pred = elastic_net_tuned.predict(X_test)

# Accuracy Scores
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)


# Show results
print(f"ElasticNet Tuned Regression R2: {r2}")
print(f"ElasticNet Tuned Regression MSE: {mse}")

"""#### 7.2.4.3 ElasticNet Learning Curve <a id=66></a>"""

plot_learning_curve(elastic_net_tuned, X, y)

"""**the model seems to benefit from more training examples, as indicated by the decreasing cross-validation MSE. The slight increase in training MSE with more data might be a concern, but since the cross-validation MSE is still decreasing, the model is likely generalizing well. To further improve the model, one might consider investigating the cause of the increase in training MSE, such as potential issues with the additional training data or the model's complexity.**

**Based on this analysis, the model does not appear to be overfitting significantly**

#### 7.2.4.4 ElasticNet Model Performance <a id=67></a>
"""

evaluate_regression_model(elastic_net_tuned)

visualize_residuals(elastic_net_tuned)

y_train_pred = elastic_net_tuned.predict(X_train)
y_test_pred = elastic_net_tuned.predict(X_test)

train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

print("Train MSE:", train_mse)
print("Test MSE:", test_mse)
print("Train R-squared:", train_r2)
print("Test R-squared:", test_r2)

"""#### 7.2.4.5 Analysis Output <a id=68></a>

1. Train MSE (Mean Squared Error) is 0.0319, which is relatively low, indicating a good fit on the training data.
2. Test MSE is 0.0341, which is only slightly higher than the Train MSE. A large difference between Train and Test MSE would indicate overfitting, but in this case, the difference is small.
3. Train R-squared is 0.9681, which is very close to 1, suggesting that the model explains most of the variance in the training data.
4. Test R-squared is 0.9661, which is also very high and close to the Train R-squared value. A significant drop in R-squared from Train to Test would indicate overfitting, but here the drop is minimal.
"""